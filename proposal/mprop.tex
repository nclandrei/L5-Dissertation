\documentclass{mprop}
\usepackage{graphicx}

\usepackage{natbib}

% alternative font if you prefer
%\usepackage{times}

% for alternative page numbering use the following package
% and see documentation for commands
%\usepackage{fancyheadings}


% other potentially useful packages
%\uspackage{amssymb,amsmath}
%\usepackage{url}
%\usepackage{fancyvrb}
%\usepackage[final]{pdfpages}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Software Ticket Quality}
\author{Andrei-Mihai Nicolae}
\date{December 18th 2017}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}\label{intro}

% 2 - 3 pages tops

\begin{itemize}
\item Background briefly explain the context of the project problem
\item Problem statement and justification. Clearly state the problem to be addressed in your forthcoming project.
  Explain why it would be worthwhile to solve this problem.

\item Research questions and contributions
\item Structure of the rest of the document
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background Survey}
% 10 - 12 pages

\subsection{Study Design}

In order to conduct a thorough background survey, several topics needed to be 
explored, among which the most important were:
  \begin{itemize}
    \item data quality and metrics;
    \item issue/ticket quality;
    \item waste in software projects;
    \item sentiment analysis.
  \end{itemize}

Firstly, we will deal with a large volume of tickets and information extracted 
from software projects and the tools that revolve around them, such as issue 
trackers and version control systems. Thus, \emph{data quality} is an important 
topic that needed to be explored in order to infer appropriate conclusions. 

We analyzed data quality mainly in software organizations and looked at what metrics
are most useful when trying to perform measurements. We also tried to find different
methodologies that either measure data quality or calculate the impact of poor data
quality on the software project. 

Secondly, probably the topic of most interest to our research is \emph{issue quality}.
This field is mainly concerned with software tickets and their characteristics: how
important are issue trackers to a software project, what makes for a good bug report,
what impact does bad triaging have etc. As we are interested in finding out what
features are most important in improving the quality of a ticket, the issue quality
literature I have reviewed proved to be beneficial and provided valuable insight into
how other researchers tried to approach this problem.

The third topic was waste in software projects and how it impacts them. We needed to
look into various communication and tool-related waste in software projects because
having a project with a considerable amount of waste might affect our results and
compromise the evaluation process. We investigated researches that looked into what
are the main types of waste in software projects and how it can be avoided so that
we could apply the findings into selecting the projects.

Last but not least, we did a literature review on sentiment analysis as well 
because we will inspect, among other components of a ticket, comments - we want 
to analyze the textual representation of comments and see if the overall 
sentiment influences the time it takes to close a ticket (i.e. maybe a negative 
conversation in comments might increase the time it takes to solve the issue).

The main sources of information when searching for papers were Google Scholar, 
ACM digital library and IEEE Xplore digital library. The first step in the 
literature review process was finding relevant papers that could aid us in
our research and splitting them into the categories mentioned above. There were
a couple of key search phrases that I started with (along with slight variations):
  \begin{itemize}
    \item software data quality;
    \item software data quality metrics;
    \item data quality in software projects;
    \item issue quality in software projects;
    \item ticket quality software;
    \item sentiment analysis;
    \item software project communication waste.
  \end{itemize}

Then, after reading the papers retrieved when searching for the above-mentioned
phrases, I also applied the snowballing research technique as per Dr. Storer's 
advice. This extra step was helpful and allowed me to easily find linked 
papers and correlate them in this literature review.

Moreover, when reading papers, one key aspect for selecting and including it
in the literature review was citation count. Thus, after finding the most 
cited papers, I then checked to see the journal where it was published so that
we would only have material from the most important journals.

Even though the literature review contains around 40 papers, there were
around 80 papers that were reviewed but roughly half of them were excluded 
because they eventually proved to not be so relevant to our research. 

\subsection{Data Quality and Metrics}

Several authors have investigated the \emph{meaning} of data quality and
what characteristics define it.

\citet{bachmann2009software} conducted a thorough investigation of several
software projects, both open source (5 projects) as well as closed source (1 
project), in order to infer what determines if it has quality. They selected 
various sources of information, among which bug tracking 
databases and version control systems logs, and examined SVN logs, CVS logs
and the content of the bug tracker databases, in the end trying to link 
the logs with the bug tracker contents as they are not integrated by default. 
On the other hand, \citet{strong1997data} conducted a qualitative analysis
instead of quantitative by collecting data from 42 data quality projects by 
interviewing custodians, customers and managers. They analyzed each project
using the data quality dimensions as content analysis codes.

The first study came to several conclusions, among which:
  \begin{itemize}
    \item closed source software projects usually exhibit better data quality 
    (i.e. better average number of attachments, better average status changes, 
    better average number of commits per bug report);
    \item reduced traceability between bug reports and version control logs
    due to scarce linkage between the two;
    \item open source projects exhibit reduced quality in change logs as,
    for example, the Eclipse project has over 20\% empty commit messages.
  \end{itemize}

However, the second study reached the conclusion that representational data 
quality dimensions are underlying causes of accessibility data quality problem
patterns. The authors also found out that three underlying causes for users'
complaints regarding data not supporting their tasks are incomplete data, 
inadequately defined or measured data and data that could not be appropriately
aggregated.

The work of \citet{kitchenham1996software} looks specifically at defining
quality in a software projects, as well as finding who are the people in charge of 
this and how should they approach achieving it. They tried to define quality 
in software projects and analyze techniques that measure such metrics by looking
at other models proposed in different other researches, such as McCall's quality
model or ISO 9126. However, they learned that quality is very hard to define and
there are various factors which need to be taken into consideration, such as the
business model of the company, the type of the software project (e.g. safety 
critical, financial sector) or the actors who are involved and how they coordinate
the software activities.

One other piece of data that is probably the most crucial in a software project
is code, and \citet{stamelos2002code} tried to discuss and examine the quality
of the source code delivered by open source projects. They used a set of tools
that could automatically inspect various aspects of source code, looking at
the 6th release of the OpenSUSE project and its components defined by C
functions. The research's results show that Linux applications have high quality 
code standards that one might expect in an open source repository, but 
the quality is lower than the one implied by the standard. More than half
of the components were in a high state of quality, but on the other hand, 
most lower quality components cannot be improved only by applying some
corrective actions. Thus, even though not all the source code was in 
an industrial standards shape, there is definitely room for further 
improvement and open source repositories proved to be of good quality.

After defining what data quality is and what are its characteristics, the next 
step would be how to measure data or information quality in a project. 

There are several research papers that try to find the answer, among which the 
work of \citet{lee2002aimq}. The authors tried to define an overall model along 
with an accompanying assessment instrument for quantifying information quality 
in an organization. The methodology has 3 main steps that need to be followed 
in order to apply it successfully:
  \begin{itemize}
    \item 2 $\times$ 2 model of what information quality means to managers;
    \item questionnaire for measuring information quality along the dimensions
    found in first step;
    \item two analysis techniques for interpreting the assessments captured 
    by the questionnaire.
  \end{itemize} 
After developing the technique, they applied it at 5 different organizations and
found that the tool proved to be practical.

Compared to the methodology proposed by \citet{lee2002aimq}, the solution found
by \citet{Heinrich2007MetricsDataQuality} is quite different. Even though the 
main goals were the same, the authors used a single metric, and that is 
the metric for timeliness (i.e. whether the values of attributes still 
correspond to the current state of their real world counterparts and whether 
they are out of date). Thus, they applied the metric at a major German mobile 
services provider. Due to some Data Quality issues the company was having, they 
had lower mailing campaign success rates, but after applying the metrics, the 
company was able to establish a direct connection between the results of
measuring data quality and the success rates of campaigns.

\citet{nelson2005antecedents} proposed another technique for measuring data 
quality in an organization by, firstly, setting 2 main research questions:
  \begin{itemize}
    \item identify a set of antecedents that both drive information and
      system quality, as well as define the nature of the IT artifact;
    \item explore data warehousing in general, especially analytical tools,
      predefined reports and ad hoc queries.
  \end{itemize}

Then, the authors set up a model to define a tree-structured representation
of system quality and data quality, as follows:
  \begin{itemize}
    \item data quality - defined by completeness, accuracy, format and currency;
    \item system quality - defined by reliability, flexibility, integration,
    response time and accessibility;
    \item then, data and systems available are evaluated to infer 
    data satisfaction and system satisfaction (coming from customers), which
    in turn will compute the final satisfaction score of the product.
  \end{itemize}

After conducting a cross-functional survey to test the model, they learned that
the features they selected were a good indicator of overall information/data and
system quality. They also found that accuracy is the dominant determinant across
all three data warehouse technologies, followed by completeness and format. Last
but not least, another discovery was that more attention needs to be given to
differences across varying technologies.

Another important area of data quality is what methodologies or techniques can 
be applied by organizations in order to improve data quality. 
There are several studies that tried to propose such methodologies. A general 
overview of such techniques  was presented by \citet{pipino2002data} - they 
wanted to present ways of developing usable  data quality methods that 
organizations can implement in their own internal processes. After reviewing 
various techniques of assessing data quality in information systems, they reached 
the conclusion that there is no universal approach to assess data quality as it  
heavily depends on the context where it is analysed. 

One such methodology examined by \citet{pipino2002data} is the technique 
proposed by \citet{wang1998product} called Total Data Quality Management
(abbreviated TDQM). Through this method, the authors wanted to help 
organizations deliver high quality information products to information
consumers by following the TDQM cycle - define, measure, analyse and improve
information quality continuously. In order to do that, the research proposes
4 steps: clearly articulate the information product, establish several roles
that would be in charge of the information product management, teach information
quality assessment and management skills to all information products 
constituencies and, finally, institutionalize continuous information product 
improvement. After developing the methodology, the authors conclude on a 
confident note that their technique can fill a gap in the information quality
environment.

However, there are other aspects that can be investigated when trying to improve
data quality. For example, \citet{prybutok2008evaluating} tried to find if 
leadership and information quality can lead to positive outcomes in an 
e-government's data quality. They conducted a web survey to gather data and test 
their hypotheses. Afterwards, they assessed the City of Denton's e-government 
initiatives, including current plans and implementations. They learned that the 
MBNQA leadership triad (leadership, strategic planning and customer/market focus) 
had a positive impact on the IT quality triad (information, system and service 
quality). Moreover, they found out that both leadership and IT quality improved 
the benefits.

Finally, after defining, measuring and improving data quality, we also need
to be able to assess the impact poor DQ has on a company. In this area, there
are several authors that investigated various implications of low indices
of data quality.

One such research is the one done by \citet{gorla2010organizational}. They wanted
to examine what influences do system quality, service quality and data quality
have on an organization, as well as what effect does system quality have on
data quality. After conducting a construct measurement for the information
systems quality dimensions \citep{swanson1997maintaining} and collecting data
through empirical testing, they learned that service quality has the greatest
impact of all three quality constructs. Moreover, they found out that there is 
a linkage between system and information quality, fact that previous research
did not reveal. Last but not least, their results indicate that the 
above-mentioned quality dimensions have a significant positive influence on 
organizational impact either directly or indirectly. 

On the other hand, \citet{redman1998impact} had different findings. Even though
they did not analyze multiple types of quality, but only data quality, they 
learned that there are three main issues across most enterprises: inaccurate
data, inconsistencies across databases and unavailable data necessary for 
certain operations. Furthermore, they presented a number of impacts that
poor data quality has on enterprises: lowered customer satisfaction, increased
cost, lowered employee satisfaction, it becomes more difficult to set strategy,
as well as worse decision making.

Another research, the one of \citet{lee2003knowing}, investigated another 
aspect: whether knowing-why affects work performance and whether knowledge held
by people in different roles affect the overall work performance. After
conducting various surveys with 6 companies that served as data collection 
points, they reached the conclusion that there is vast complexity of 
knowledge at work in the data production process and if these gaps are not 
bridged it might lead to decrased data quality.

\subsection{Issue Quality}

Issue quality is the main topic of this literature review as it revolves around
software tickets and what are the characteristics of a well-written and 
informative bug report, how efficient are bug tracking systems, what defines
an efficient bug triaging process etc. 

The first and most important sub-topic of issue quality, which is the one
we will address in our own research as well, is the quality of bug reports.
There are several authors that have tried to defined what makes for a good bug
report and what components actually improve the overall quality.

\citet{bettenburg2008makes} have tried, through their work, to analyze what 
makes for a good bug report through qualitative analysis. They interviewed over
450 developers and asked them what are the most important features for them in
a bug report that help them solve the issue quicker. They reached the conclusion 
that stack traces and steps to reproduce increased the quality of a bug report 
the most, followed by well-written, grammar error free summaries and 
descriptions. Last but not least, they also created a tool called CueZilla that 
could with an accuracy rate between 31 and 48\% predict the quality of a bug 
report. 

Strengthening the argument that readability matters considerably in bug report 
quality is the work of \citet{hooimeijer2007modeling}. They ran an analysis over
27000 bug reports from the Mozilla Firefox project, looking at self-reported 
severity, readability, daily load, submitter reputation and changes over time. 
After running the evaluation, they not only found out about the importance of
readability in bug reports, but also that attachment and comment counts are 
valuable for faster triaging and that patch count, for example, does not
contribute in the same manner as the previous two.

Another research that agrees that stack traces are helpful in solving bug
reports faster is the one performed by \citet{schroter2010stack}. They conducted
the whole experiment on the Eclipse project. They first extracted the stack
traces using the infoZilla tool(\citep{bettenburg2008extracting}), followed by
linking the stack traces to changes in the source code (change logs) by mining
the version repository of the Eclipse project. The results were that 
around 60\% of the bugs that contained stack traces in their reports were fixed
in one of the methods in the frame, with 40\% being fixed in exactly the first
stack frame.

Following on the conclusions of \citet{bettenburg2008makes} we have found out
that the investigations undertaken by \citet{bettenburg2007quality} reveal 
very similar results. They performed the same type of evaluation method (i.e. 
interviews with developers) and confirmed that steps to reproduce and stack traces
are the most important features in a bug report that help developers solve the
issue quicker.

Other interesting findings regarding the quality of a software project's tickets
are the ones elicited in \citet{bettenburg2008duplicate}. The authors examined
whether duplicate bug reports are actually harmful to a project or they add 
quality. They collected big amount of data from the Eclipse project and ran
various kind of textual and statistical analysis on the data to find answers.
They reached the conclusion that bug duplicates contain information that is not
present in the master reports. This additional data can be helpful for developers
and it can also aid automated triaging techniques.

However, in order to model the quality of a bug report, one needs to be able to
successfully extract various types of information from such a report and, if 
possible, link them to the source code of the project (i.e. source code fragments
in bug report discussions should be linked to the corresponding sections in the 
actual code) or other software artifacts. There are several researches that tried 
to analyze such techniques and one of them is the work of 
\citet{bettenburg2012using}. The authors created a tool that could parse a bug 
report (using fuzzy code search) and extract source code that could then be
matched with exact locations in the source code, in the end producing a 
traceability link (i.e. tuple containing a clone group ID and file paths 
corresponding to the source code files). Evaluation showed an increase of
roughly 20\% in total traceability links between issue reports and source code 
when compared to the current state-of-the-art technique, change log analysis.

\citet{bettenburg2012using} also made use of a tool developed by 
\citet{bettenburg2008extracting} called infoZilla. This application can
parse bug reports and correctly extract patches, stack traces, source code and
enumerations. When evaluating it on over 160.000 Eclipse bug reports, it proved to 
have a very high rate of over 97\% accuracy.

However, \citet{kim2013should} propose a rather different technique than the one
exposed in \citet{bettenburg2012using}. The authors have employed a two-phase
recommendation model that would locate the necessary fixes based on information
in bug reports. Firstly, they did a feature extraction on the bug reports (e.g. 
extract description, summary, metadata). Then, in order to successfully predict
locations, this model was trained on collected bug reports and then, when given
a new bug report, it would try to localize the files that need to be changed 
automatically. Finally, the actual implementation was put into place, and that
is the two phase recommendation model, composed of binary (filters out 
uninformative bug reports before predicting the files to fix) and multiclass (
previously filtered bug reports are used as training data and only after that
new bug reports can be analyzed and files to be changed recommended). The 
overall accuracy was above the one achieved by \citet{bettenburg2012using}, 
being able to rank over 70\%, but only as long as it recommended \emph{any} 
locations.

One other type of information that could be extracted and used for valuable
purposes is the summary of a bug report, as shown in the work of 
\citet{rastkar2010summarizing}. The authors wanted to determine if bug reports
could be summarized effectively and automatically so that developers would
need only to analyze summaries instead of full tickets. Firstly, they asked
university students to volunteer to annotate the bug reports collected from
various open sources projects (i.e. Eclipse, Mozilla, Gnome, KDE) by writing
a summary of maximum 250 words in their own sentences. These human-produced 
annotations were then used by algorithms to learn how to effectively summarize a 
bug report.  Afterwards, the authors asked the end users of these bug reports, the 
software developers, to rate the summaries against the original bug reports.
They eventually learned that existing conversation-based extractive summary
generators trained on bug reports produce the best results.

A third technique for linking source code files from the textual description of a 
bug report is the one proposed by \citet{zhou2012should}. The authors
firstly performed a textual analysis, looking for similarities between the bug 
report description and the source code files. Then, it analyzed previous bugs in 
the repository to find the most similar ones, thus being able to find which files 
ought to be changed. Lastly, it assigned scores to similar files, the ones with 
bigger sizes obtaining higher scores as they are more likely to contain bugs. They 
collected the necessary data from Bugzilla projects and then performed the 
evaluation. The results of the evaluation phase were positive: the bug locator
can locate a large percentage of bugs analyzing just a small set of source code
files. Moreover, similar bugs can improve the localization accuracy only to a 
certain extent, while the locator outperformed every other competitor on a 
multitude of projects. 

Having discussed automated linkage between source code files and bug reports, 
\citet{fischer2003analyzing} presents a way to track certain features from the
data residing in tickets. The authors employed a method to track features by 
analyzing and relating bug report data filtered from a release history database.
Features are then instrumented and tracked, relationships of modification and 
problem reports to these features are established, and tracked features are
visualized to illustrate their otherwise hidden dependencies. They managed to 
successfully visualize the tracked features and illustrate their non apparent
dependencies which can prove very useful in projects with a large number of
components.

Even though we have discussed about quality in bug reports, how it can be 
identified and what state-of-the-art techniques can be used to extract and 
link various components from tickets, we must also investigate if the platform 
where these bug reports 
reside (i.e. bug trackers) are properly fitting the software environment. 
The work of \citet{just2008towards} tries to find the answer to this question.
The authors launched a survey to developers from Eclipse, Mozilla and Apache
open source projects from which they received 175 comments back. Then, they
applied a card sort in order to organize the comments into hierarchies to 
deduce a higher level of abstraction and identify common patterns. In the end,
they ranked the top suggestions as follows:
  \begin{itemize}
    \item provide tool support for users to collect and prepare information that 
      developers need;
    \item find volunteers to translate bug reports filed in foreign languages;
    \item provide different user interfaces for each user level and give cues to 
      inexperienced reporters on what to report and best practices;
    \item reward reporters when they do a good job;
    \item integrate reputation into user profiles to mark experienced reporters;
    \item provide powerful and easy to use tools to search bug reports;
    \item encourage users to submit additional details (provide tools for merging 
      bugs).
  \end{itemize}

One other important aspect when analyzing the quality of a bug report is the
time it takes to triage it. Triaging is the process of analyzing issues and 
based on the information available, determine their severity and assign them to the
most qualified available developer to fix them. If the ticket would be valuable
and have an increased index of quality, the triaging process would be quicker and,
thus, reduce the cost for the organization. There are several researches that 
are concerned with this field and how gaps can be bridged.

One such example is the work of \citet{anvik2006automating}. The authors tried
to automate the process of bug report assignment by employing a machine learning 
algorithm which, after receiving as input various types of information (e.g. 
textual description of bug, bug component, OS, hardware, software version, 
developer who owns the code, current workload of developers, list of developers 
actively contributing), would recommend the best developers to work on the ticket. 
After the tool was evaluated, it was found that it could actually be deployed
in the software industry so that it could be used by organizations.

Working on the previous tool, the same authors proposed a new solution to automatic
bug report assignment through the work presented in \citet{anvik2011reducing}.
They increased the precision and recall of the algorithm and then, in order to 
test it, they implemented a proxy to the actual web service that's providing the
issue repository. Eventually, they also found out that the impact of poor
software project management, including bug triaging, on software projects can
reach alarming levels.

A similar research, but which looks at the free form of the ticket only, is
the technique presented in \citet{anvik2006should}. The authors applied a 
supervised machine learning algorithm on the repositories to learn which 
developers were best suited for specific tasks, thus when a new bug report would 
come in, a small set of people would be selected. In order to train the algorithm,
they looked at Bugzilla repositories and selected the free text form of tickets, 
trying to label similar ones based on textual similarities. Once the tickets were
labeled and grouped for specific developers, the algorithm would then be able
to present the triager the set of developers suitable to fix the bug. Even though
this approach, compared to \citet{anvik2006automating} or 
\citet{anvik2011reducing}, does not reach the same levels of accuracy, it is much
simpler to implement and evaluate.

However, a completely different approach is the one presented by 
\citet{jeong2009improving} that looks at both bug report assignment as well as bug 
tossing (i.e. re-assign tickets from one developer to another). They wanted, 
through their tossing graph approach based on the Markov property, to both 
discover developer networks and team structures, as well as help to better assign 
developers to bug reports. They analyzed 145.000 bug reports from Eclipse and 
300000 from Mozilla and then, using statistical analysis on the bug reports in 
order to find evolution histories and changes throughout the lifetime of the 
reports, they created the bug tossing graph. The research shows that it takes a 
long time to assign and toss bugs and, additionally, they learned that their model 
reduces tossing steps by up to 72\% and improved the automatic bug assignment by 
up to 23\%.

Another approach to bug report assignee recommendation is the one based on
activity profiles, proposed by \citet{Naguib2013BugReportAssignee}. The authors 
employed an algorithm using activity profiles (i.e. assign, review, resolve 
activity of the developer) such that, after detecting the prior
experience, developer's role, and involvement in the project, it could
recommend who should fix a specific bug. The average accuracy was around
88\%, much higher than other techniques, including the ones mentioned previously. 

However, bug report assignment is not the only part of bug triaging that needs
investigation. \citet{lamkanfi2010predicting} show a method through which one
can predict the severity of a reported bug (i.e. assign story points to the 
ticket) automatically. The authors conducted their research on the Mozilla,
Eclipse and GNOME projects and divided their method into 5 steps:
  \begin{itemize}
    \item extract and organize bug reports;
    \item pre-process bug reports - tokenization, stop word removal and stemming;
    \item train the classifier on multiple datasets of bug reports - 70\% 
      training and 30\% evaluation;
    \item apply the trained classifier on the evaluation set.
  \end{itemize}
The conclusions they drew from running the evaluation process were rather 
interesting:
  \begin{itemize}
    \item terms such as deadlock, hand, crash, memory or segfault usually indicate
      a severe bug; however, there are other terms that can indicate the opposite,
      such as typos;
    \item when analyzing the textual description they found that using simple
      one line summaries resulted in less false positives and increased precision
      levels;
    \item the classifier needs a large dataset in order to predict correctly;
    \item depending on how well a software project is componentized, one can 
      use a predictor per component rather than an universal one.
  \end{itemize}

\subsection{Measuring Cost and Waste in Software Projects}

\textbf{Secret Life of Bugs}\cite{aranda2009secret}:
\begin{itemize}
\item What were the goals?
  The paper tries to understand and analyze common bug fixing coordination
  activities. Another goal of the paper was to analyze the reliability of
  repositories in terms of software projects coordination and propose
  different directions on how to implement proper tools.
\item What was the method?
  They executed a field study which was split into two parts:
  \begin{itemize}
  \item firstly, they did an exploratory case study of bug repos histories;
  \item secondly, they conducted a survey with professionals (i.e.\ testers,
    developers).
  \end{itemize}
  All data and interviews were conducted using Microsoft bug repositories and
  employees.
\item What did they learn?
  They learned that there are multiple factors which influence the coordination
  activities that revolve around bug fixing, such as organizational, social and
  technical knowledge, thus one cannot infer any conclusions only by automatic
  analysis of the bug repositories. Also, through surverying the professionals,
  they reached the conclusion that there are 8 main goals which can be used for
  better tools and practices:
  \begin{itemize}
  \item probing for ownership;
  \item summit;
  \item probing for expertise;
  \item code review;
  \item triaging;
  \item rapid-fire emailing;
  \item infrequent, direct email;
  \item shotgun emails.
  \end{itemize}
\item Relevance to our work: this paper is one of the key papers for our
  research paper.
\end{itemize}

\textbf{Analysis of Software Cost Estimation}\cite{grimstad2006framework}:
\begin{itemize}
  \item \textbf{What were the goals?}
    The authors are trying to show that poor estimation analysis techniques 
    in software projects will lead to wrong conclusions regarding cost 
    estimation accuracy. Moreover, they also propose a framework for 
    better analysis of software cost estimation error.
  \item \textbf{What was the method?}
    They approached a real-world company where they conducted analysis on
    their cost estimation techniques.
  \item \textbf{What did they learn?}
    They learned that regular, straight-forward types of cost estimation analysis
    techniques error lead them to wrong conclusions. 
  \item \textbf{Relevance to our work}: it showed us that we need to be careful
    when selecting techniques for cost estimation in our own research.
\end{itemize}

\textbf{Effects of process maturity on quality, cycle time and effort in software product development}\cite{harter2000effects}:
\begin{itemize}
  \item \textbf{What were the goals?}
  The research tries to investigate the relationship between process maturity, quality, cycle time and effort in software projects. The authors are mainly trying to prove/reject a couple of hypotheses: higher levels of process maturity lead to higher product quality in software projects, higher levels of process maturity are associated with increased cycle time in software products, higher product quality is associated with lower cycle time, higher levels of process maturity lead to increased development efforts in the project and higher product quality is associated with lower development effort.
  \item \textbf{What was the method?}
  In order to test their hypotheses, the authors examined data coming from 30 projects belonging to the systems integration division from a large IT company. The process improvement data was collected via external divisions and by government agencies to provide independent assessments of the firmâ€™s software development processes. Then, they analyzed a couple of key variables and see how they interact: process maturity, product quality, cycle time, development effort, product size, domain/data/decision complexity and requirements ambiguity.
  \item \textbf{What did they learn?}
  They learned that the main features they inspected are additively separable and linear. Moreover, they found out that higher levels of process maturity are associated with significantly higher quality, but also with increased cycle times and development efforts. On the other hand, the reductions in cycle time and effort resulting from improved quality outweigh the marginal increases from achieving more process maturity.
  \item \textbf{Relevance to our work}:
  It can aid our research through better understanding of how various factors can influence the software development processes, thus giving more value to our own research (i.e. being able to automatically determine issue quality could improve better issue writing guidelines/better rules to be enforced, thus reduced triaging and development costs). 
\end{itemize}

\textbf{Waste Identification}\cite{Korkala2014WasteIdentification}:
\begin{itemize}
  \item \textbf{What were the goals?}
    The paper had two main goals: 
    \begin{itemize}
      \item a means to identify communication waste in agile software projects 
        environments;
      \item types of communication waste in agile projects.
    \end{itemize}
  \item \textbf{What was the method?}
    The authors collaborated with a medium-sized American software company
    and conducted a series of observations, informal discussions, documents 
    provided by the organization, as well as semi-structured interviews. Moreover,
    the data collection for waste identification was split into 2 parts:
      \begin{itemize}
        \item \textbf{pre-development}: occured before the actual implementation
          begun (e.g.\ backlog creation);
        \item \textbf{development}: happened throughout the implementation
          process (e.g.\ throughout sprints, retrospectives, sprint reviews,
          communication media).
      \end{itemize}
  \item \textbf{What did they learn?}
    They realized the communication waste can be divided into 5 main categories:
      \begin{itemize}
        \item lack of involvement;
        \item lack of shared understanding;
        \item outdated information;
        \item restricted access to information;
        \item scattered information.
      \end{itemize}
    Also, they learned that their way of identifying these types of waste was
    quite efficient and they even recommend it to companies if they'd like
    to conduct such processes internally.
  \item \textbf{Relevance to our work}: the waste identification process can
    be applied to our work so that we can identify possible causes to poor 
    quality tickets.
\end{itemize}

\textbf{Software Development Waste}\cite{sedano2017software}:
\begin{itemize}
  \item \textbf{What were the goals?}
    The main goal of the paper was to identify main types of waste in software
    development projects.
  \item \textbf{What was the method?}
    They conducted a participant-observation study over a long period of time
    at Pivotal, a consultancy software development company. They also interviewed
    multiple engineers and balanced theoretical sampling with analysis to achieve
    the conclusions.
  \item \textbf{What did they learn?}
    They found out there are nine main types of waste in software projects:
    \begin{itemize}
      \item building the wrong feature or product;
      \item mismanaging backlog;
      \item extraneous cognitive load;
      \item rework;
      \item ineffective communication;
      \item waiting/multitasking;
      \item solutions too complex;
      \item psychological distress.
      \item \textbf{Relevance to our work}: this paper complements the previous 
    one on waste identification\cite{Korkala2014WasteIdentification}.
    \end{itemize}
\end{itemize}

\textbf{Waste in Kanban Projects}\cite{ikonen2010exploring}:
\begin{itemize}
  \item \textbf{What were the goals?}
    The authors are trying to find the main sources of waste in Kanban software
    development projects and categorize/rank them based on severity.
  \item \textbf{What was the method?}
    A controlled case study research was employed in a company called Software
    Factory. They conducted semi-structured interviews with 5 of the team
    members both in the beginning in order to collect data as well as at the end 
    of the whole process to categorize the seven types of waste found. Moreover,
    they also measured the overall success of the project based on Shenar's
    techniques (first-second-third-fourth; project efficiency-imapct on the
    customer-business success-preparing for the future).
  \item \textbf{What did they learn?}
    They reached two main findings:
      \begin{itemize}
        \item they found 7 types of waste throughout the project at various
          development stages:
            \begin{itemize}
              \item partially done work;
              \item extra processes;
              \item extra features;
              \item task switching;
              \item waiting;
              \item motion;
              \item defects.
            \end{itemize}
        \item they reached the conclusion that they couldn't explain the success
          of the project even though waste was found.
      \end{itemize}
  \item \textbf{Relevance to our work}: this work completes the findings 
    from the previous work presented as most of the projects we will work with
    will be Agile, thus Kanban-based in terms of issue management.
\end{itemize}

\subsection{Sentiment Analysis}

\textbf{Thumbs Up or Thumbs Down}\cite{turney2002thumbs}:
\begin{itemize}
  \item \textbf{What were the goals?}
    The main goal of the paper is to detect the overall sentiment transmitted
    through reviews of various types.
  \item \textbf{What was the method?}
    The author created an unsupervised machine learning algorithm that was 
    evaluated on more than 400 reviews on Epinions on various kinds of markets 
    (e.g.\ automobiles, movie). The algorithm implementation was divided 
    into three steps:
      \begin{itemize}
        \item extract phrases containing adjectives or adverbs;
        \item estimate the semantic orientation of the phrases;
        \item classify the review as recommended or not recommended based on
          the semantic orientation calculated at previous step.
      \end{itemize}
  \item \textbf{What did they learn?}
    One thing the author learned that different categories will yield different 
    results. For example, the automobile section on Epinions ranked much higher, 
    84\%, compared to movie reviews, which had an accuracy of 65.83\%. Moreover, 
    most pitfalls of the algorithm could be attributed to multiple factors, such 
    as not using a supervised learning system or limitations of PMI-IR.
  \item \textbf{Relevance to our work}: the method can be applied for extracting
    sentiments from the tickets (description and comments) we will use 
    in our own research.
\end{itemize}

\textbf{Recognizing Contextual Polarity}\cite{wilson2005recognizing}:
\begin{itemize}
  \item \textbf{What were the goals?}
    The paper's main goal is to find efficient ways to distinguish between 
    contextual and prior polarity.
  \item \textbf{What was the method?}
    They used a two step method that used machine learning and a variety of
    features. The first step classified each phrase which had a clue as either
    neutral or polar, followed by taking all phrases marked in the previous step
    and giving them a contextual polarity (e.g.\ positive, negative, 
    both, neutral).
  \item \textbf{What did they learn?}
    Through the method the authors employed, they managed to automatically 
    identify the contextual polarity. As most papers were only looking at the 
    sentiment extracted from the overall document, they managed to get valuable 
    results from looking at specific words and phrases.
  \item \textbf{Relevance to our work}: when analyzing the description and
    comments of the ticket, we can use their method for infering the sentiment 
    transmitted probably more accurately than the technique used in 
    Turney's paper\cite{turney2002thumbs}.
\end{itemize}

\subsection{Conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Approach}

% 3- 4 pages 

What are we actually going to do?

Research questions, outline experimental design, independent variables, dependent variables.



state how you propose to solve the software development problem. Show that your proposed approach is feasible, but identify any risks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Work Plan}

% 1 - 2 pages  structure as work packages (don't bother with a chart).

show how you plan to organize your work, identifying intermediate deliverables and dates.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{plainnat}
\bibliography{mprop}
\end{document}
