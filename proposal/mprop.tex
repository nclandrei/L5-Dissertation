\documentclass{mprop}
\usepackage{graphicx}

\usepackage{natbib}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Measuring Software Ticket Quality using Statistical Analysis}
\author{Andrei-Mihai Nicolae}
\date{December 18th 2017}
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}\label{intro}
% 2 - 3 pages tops

\subsection{Background}

Software engineering, compared to other engineering fields, such as
mechanical or electrical engineering, is much more abstract \citep{brooks1995mythical}- while 
performing maintenance on a car engine or on an electric panel,
you can actually touch the components, they are physical. However, in
computer science and the engineering of software, you cannot see the whole picture.
Of course one can actually open up a laptop or PC and see and touch the
components, but the software, what is actually displayed to the user, 
is a multitude of layers on top of each other: it starts with a number of characters in some file;
then, that file gets compiled or interpreted to machine
code by other components which run on the operating system; then, these components
can communicate with the kernel (i.e. the component that sits exactly
between the OS and the hardware) and eventually perform the final step - send 
different 1s and 0s encoded into electrical signals to the CPU and various
other hardware components.

As previously mentioned, there are many levels of abstraction that computers expose to 
their human operators, thus developing software that runs and performs as expected
in agreement with its requirements is not trivial. Therefore, developers have
created software called \emph{issue tracking systems} that can manage
and track \emph{tickets} of various forms such as bug 
reports, feature requests etc. The aforementioned tickets can vary greatly
and issue tracking systems usually allow developers to include many 
types of information, such as:
  \begin{itemize}
    \item stack traces;
    \item steps to reproduce a bug;
    \item summary and description;
    \item story points (e.g. how many units of time does it take one
      developer to mark the ticket as resolved or completed).
  \end{itemize}
In order to create such tickets, the end users or the developers 
themselves notice certain flaws in the software or propose new features
to be implemented. Then, they head to the issue tracking system and 
create tickets containing the necessary information.
However, we are facing a main issue here, and that is understanding the
key elements the developers look for when fixing a bug or implementing a 
new feature.

Therefore, a software engineering issue still not completely resolved
is how can one create valuable, high quality tickets.

\subsection{Problem Statement}

This research is investigating the features of a high quality ticket.
More specifically, it is looking into the various components 
of a ticket (e.g. summary, description, comments) and their characteristics
in order to infer what combination of such components or what specific
attributes help developers close the ticket quicker (i.e. time between
ticket creation and marked as CLOSED or RESOLVED). 

We believe that this study is valuable to the software engineering 
community because solving the issue/ticket quality problem will help
reduce both cost and effort in companies worldwide. 
Having a clear understanding of what attributes make for 
a high quality ticket will lead the project maintainers to write  
specific guidelines in the ticket creation form (maybe even create an 
automated system) that will help reporters include the most
important details and structure them accordingly (e.g. for a specific
type of project it might be essential to include stack traces so that
developers can locate the bugs quicker), which will in turn reduce
overall costs and developers/triagers' effort.

Furthermore, our study might also increase the likelihood of people
reporting bugs or asking for more specific features more often. Not having specific 
guidelines or an automated system when creating a ticket many 
non-technical users might be put off and give up creating the report. 
However, if the company would put up a form where all the information
needed can be very easily inserted, users might be more inclined 
to report bugs and create feature requests, thus increasing the 
software's overall quality on the long term.

\subsection{Research Questions and Contributions}

The main research question this study is trying to answer is 
what makes for a valuable, high quality ticket?

However, as this is a broad research question, we have divided it into
three sub-research questions which, when combined, would yield the 
answer to our main research question:
  \begin{itemize}
    \item are there components which singularly affect the quality
      of a ticket (e.g. well written summary) or only combinations of them 
      (e.g. stack traces together with steps to reproduce)?
    \item how much do specific components or combinations of them 
      affect the quality of a ticket?
    \item can quality be inferred solely from the ticket or do we also
      need to consider the surrounding context (e.g. details related to
      the version control system)?
  \end{itemize}

Even though there have been many studies related to issue/ticket quality,
most of them have looked at it qualitatively, not quantitatively (e.g. 
\citet{bettenburg2008makes}, \citet{bettenburg2007quality}). This is 
the first contribution of our study - we will explore the area of 
ticket quality from a statistical point of view, not through interviews
or other human-centered data gathering (i.e. we want to inspect several
open source repositories and collect tickets through the issue tracking
system's APIs).

Moreover, another contribution is that,
even though most studies so far have looked solely at ticket 
features/components, we want to inspect both tickets and other components
of the software project, such as version control system and how various
attributes affect the time-to-close for a particular ticket. 

The third contribution is a proposed model for bug reports and/or feature requests.
After collecting data and finding which components together with their afferent attributes
make for a high quality ticket, we will propose a model for best practices
in terms of creating tickets.

\subsection{Proposal Structure}

The rest of this project proposal is structured as follows:
  \begin{itemize}
    \item Section \ref{background_survey} presents both the study design 
      and the background survey, which in turn is divided into three 
      subsection:
        \begin{itemize}
          \item subsection \ref{data_quality} describes the literature
            review on data quality and metrics;
          \item subsection \ref{issue_quality} describes the background
            survey on issue/ticket quality;
          \item subsection \ref{waste} elicits literature on 
            waste in software projects;
        \end{itemize}
    \item Section \ref{proposed_approach} describes how we would like 
      to conduct the research;
    \item Section \ref{work_plan} shows how we broke down the work
      so that we keep a consistent pace throughout the lifecycle of 
      the research.
  \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background Survey}\label{background_survey}
% 10 - 12 pages

\subsection{Study Design}

In order to conduct a thorough background survey, several topics needed to be 
explored, among which the most important were:
  \begin{itemize}
    \item data quality and metrics;
    \item issue/ticket quality;
    \item waste in software projects;
    \item sentiment analysis.
  \end{itemize}

Firstly, we will deal with a large volume of tickets and information extracted 
from software projects and the tools that revolve around them, such as issue 
trackers and version control systems. Thus, \emph{data quality} is an important 
topic that needed to be explored in order to infer appropriate conclusions. 

We analyzed data quality mainly in software organizations and looked at what metrics
are most useful when performing measurements. We also found different
methodologies that either measure data quality or calculate the impact of poor data
quality on the software project. 

Secondly, probably the topic of most interest to our research is \emph{issue quality}.
This field is mainly concerned with software tickets and their characteristics: how
important are issue trackers to a software project, what makes for a good bug report,
what impact does bad triaging have etc. As we are interested in finding out what
features are most important in improving the quality of a ticket, the issue quality
literature reviewed proved to be beneficial and provided valuable insight into
how other researchers tried to approach this problem.

The third topic was waste and its impact on software projects. We needed to
look into various communication and tool-related waste in software projects because
having a project with a considerable amount of waste might affect our results and
compromise the evaluation process. We investigated researches that looked into what
the main types of waste are and how it can be avoided so that
we could apply the findings into selecting the projects.

Last but not least, we did a literature review on sentiment analysis as well 
because we will inspect, among other components of a ticket, comments - we want 
to analyze the textual representation of comments and see if the overall 
sentiment influences the time it takes to close a ticket (i.e. a conversation
might unnecessarily delay solving the ticket).

The main sources of information when searching for papers were Google Scholar, 
ACM digital library and IEEE Xplore digital library. The first step in the 
literature review process was finding relevant papers that could aid us in
our research and splitting them into the categories mentioned above. The
key search phrases used (with slight variations) are as follows:
  \begin{itemize}
    \item software data quality;
    \item software data quality metrics;
    \item data quality in software projects;
    \item issue quality in software projects;
    \item ticket quality software;
    \item sentiment analysis;
    \item software project communication waste.
  \end{itemize}

The snowballing research technique was also applied when retrieving papers for the
background review. Citation count and publication journal were two important aspects 
when selecting papers - the most cited papers, published in the top quality journals were reviewed.

\subsection{Data Quality and Metrics}\label{data_quality}

Several authors have investigated the \emph{meaning} of data quality and
what characteristics define it.

\citet{bachmann2009software} conducted a thorough investigation of several
software projects, both open source (5 projects) as well as closed source (1 
project), in order to infer what determines their quality. They selected 
various sources of information, among which bug tracking 
databases and version control systems logs, and examined SVN logs, CVS logs
and the content of the bug tracker databases, in the end trying to link 
the logs with the bug tracker contents as they are not integrated by default. 
A different approach was taken by \citet{strong1997data} who conducted a qualitative analysis
instead of quantitative by collecting data from 42 data quality projects by 
interviewing custodians, customers and managers. They analyzed each project
using the data quality dimensions as content analysis codes.

The first study came to several conclusions, among which:
  \begin{itemize}
    \item closed source software projects usually exhibit better data quality 
    (i.e. better average number of attachments, better average status changes, 
    better average number of commits per bug report);
    \item reduced traceability between bug reports and version control logs
    due to scarce linkage between the two;
    \item open source projects exhibit reduced quality in change logs as,
    for example, the Eclipse project has over 20\% empty commit messages.
  \end{itemize}

However, the second study reached the conclusion that representational data 
quality dimensions are underlying causes of accessibility data quality problem
patterns. The authors also found out that three underlying causes for users'
complaints regarding data not supporting their tasks are incomplete data, 
inadequately defined or measured data and data that could not be appropriately
aggregated.

The work of \citet{kitchenham1996software} looks specifically at defining
quality in a software project, as well as finding who the people in charge of this 
are and how they should approach achieving it. They tried to define quality 
in software projects and analyze techniques that measure such metrics by looking
at other models proposed in different researches, such as McCall's quality
model or ISO 9126. However, they learned that quality is very hard to define and
there are various factors which need to be taken into consideration, such as the
business model of the company, the type of the software project (e.g. safety 
critical, financial sector) or the actors who are involved and how they coordinate
the software activities.

One other piece of data that is probably the most crucial in a software project
is code, and \citet{stamelos2002code} tried to discuss and examine the quality
of the source code delivered by open source projects. They used a set of tools
that could automatically inspect various aspects of source code, looking at
the 6th release of the OpenSUSE project and its components defined by C
functions. The results show that Linux applications have high quality 
code standards that one might expect in an open source repository, but 
the quality is lower than the one implied by the standard. More than half
of the components were in a high state of quality, but on the other hand, 
most lower quality components cannot be improved only by applying some
corrective actions. Thus, even though not all the source code was in 
an industrial standards shape, there is definitely room for further 
improvement and open source repositories proved to be of good quality.

After defining what data quality is and its characteristics, the next 
step would be data measurement or information quality in a project. 

There are several research papers that try to find the answer, among which the 
work of \citet{lee2002aimq}. The authors tried to define an overall model along 
with an accompanying assessment instrument for quantifying information quality 
in an organization. The methodology has 3 main steps that need to be followed 
in order to apply it successfully:
  \begin{itemize}
    \item 2 $\times$ 2 model of what information quality means to managers;
    \item questionnaire for measuring information quality along the dimensions
    found in first step;
    \item two analysis techniques for interpreting the assessments captured 
    by the questionnaire.
  \end{itemize} 
After developing the technique, they applied it in 5 different organizations and
found that the tool proved to be practical.

Compared to the methodology proposed by \citet{lee2002aimq}, the solution found
by \citet{Heinrich2007MetricsDataQuality} is quite different. Even though the 
main goals were the same, the authors used a single metric, and that is 
the metric for timeliness (i.e. whether the values of attributes still 
correspond to the current state of their real world counterparts and whether 
they are out of date). Thus, they applied the metric at a major German mobile 
services provider. Due to some Data Quality issues the company was having, they 
had lower mailing campaign success rates, but after applying the metrics, the 
company was able to establish a direct connection between the results of
measuring data quality and the success rates of campaigns.

\citet{nelson2005antecedents} proposed another technique for measuring data 
quality in an organization by, firstly, setting 2 main research questions:
  \begin{itemize}
    \item identify a set of antecedents that both drive information and
      system quality, as well as define the nature of the IT artifact;
    \item explore data warehousing in general, especially analytical tools,
      predefined reports and ad hoc queries.
  \end{itemize}

Then, the authors set up a model to define a tree-structured representation
of system quality and data quality, as follows:
  \begin{itemize}
    \item data quality - defined by completeness, accuracy, format and currency;
    \item system quality - defined by reliability, flexibility, integration,
    response time and accessibility;
    \item then, data and systems available are evaluated to infer 
    data satisfaction and system satisfaction (coming from customers), which
    in turn will compute the final satisfaction score of the product.
  \end{itemize}

After conducting a cross-functional survey to test the model, they learned that
the features they selected were a good indicator of overall information/data and
system quality. They also found that accuracy is the dominant determinant across
all three data warehouse technologies, followed by completeness and format. Last
but not least, another discovery was that more attention needs to be given to
differences across varying technologies.

Another important area of data quality is what methodologies or techniques can 
be applied by organizations in order to improve it. 
There are several studies that tried to propose such methodologies. A general 
overview of such techniques  was presented by \citet{pipino2002data} - they 
wanted to present ways of developing usable  data quality methods that 
organizations can implement in their own internal processes. After reviewing 
various techniques of assessing data quality in information systems, they reached 
the conclusion that there is no universal approach to assess data quality as it  
heavily depends on the context where it is analysed. 

One such methodology examined by \citet{pipino2002data} is the technique 
proposed by \citet{wang1998product} called Total Data Quality Management
(abbreviated TDQM). Through this method, the authors wanted to help 
organizations deliver high quality information products to information
consumers by following the TDQM cycle - define, measure, analyse and improve
information quality continuously. In order to do that, the research proposes
4 steps: clearly articulate the information product, establish several roles
that would be in charge of the information product management, teach information
quality assessment and management skills to all information products 
constituencies and, finally, institutionalize continuous information product 
improvement. After developing the methodology, the authors conclude on a 
confident note that their technique can fill a gap in the information quality
environment.

However, there are other aspects that can be investigated when trying to improve
data quality. For example, \citet{prybutok2008evaluating} tested whether  
leadership and information quality can lead to positive outcomes in an 
e-government's data quality. They conducted a web survey to gather data and test 
their hypotheses. Afterwards, they assessed the City of Denton's e-government 
initiatives, including current plans and implementations. They learned that the 
MBNQA leadership triad (leadership, strategic planning and customer/market focus) 
had a positive impact on the IT quality triad (information, system and service 
quality). Moreover, they found out that both leadership and IT quality improved 
the benefits.

Finally, after defining, measuring and improving data quality, we also need
to be able to assess the impact poor DQ has on a company. In this area, there
are several authors that investigated various implications of low indices
of data quality.

One such research is the one done by \citet{gorla2010organizational}. They wanted
to examine the influences of system quality, service quality and data quality
on an organization, as well as what effect does system quality have on
data quality. After conducting a construct measurement for the information
systems quality dimensions \citep{swanson1997maintaining} and collecting data
through empirical testing, they learned that service quality has the greatest
impact of all three quality constructs. Moreover, they found out that there is 
a linkage between system and information quality, fact that previous research
did not reveal. Last but not least, their results indicate that the 
above-mentioned quality dimensions have a significant positive influence on 
organizational impact either directly or indirectly. 

On the other hand, \citet{redman1998impact} had different findings. Even though
they did not analyze multiple types of quality, but only data quality, they 
learned that there are three main issues across most enterprises: inaccurate
data, inconsistencies across databases and unavailable data necessary for 
certain operations. Furthermore, they presented a number of impacts that
poor data quality has on enterprises: lowered customer satisfaction, increased
cost, lowered employee satisfaction, it becomes more difficult to set strategy,
as well as worse decision making.

Another research, the one of \citet{lee2003knowing}, investigated another 
aspect: whether knowing-why affects work performance and whether knowledge held
by people in different roles affect the overall work performance. After
conducting various surveys with 6 companies that served as data collection 
points, they reached the conclusion that there is vast complexity of 
knowledge at work in the data production process and if these gaps are not 
bridged it might lead to decreased data quality.

\subsection{Issue Quality}\label{issue_quality}

Issue quality is the main topic of this literature review as it revolves around
software tickets and the characteristics of a well-written and 
informative bug report, how efficient are bug tracking systems, what defines
an efficient bug triaging process etc. 

The first and most important sub-topic of issue quality, which is the one
we will address in our own research as well, is the quality of bug reports.
There are several authors that have tried to defined what makes for a good bug
report and what components actually improve the overall quality.

\citet{bettenburg2008makes} analyzed what 
makes for a good bug report through qualitative analysis. They interviewed over
450 developers and asked them what are the most important features for them in
a bug report that help them solve the issue quicker. They reached the conclusion 
that stack traces and steps to reproduce increased the quality of a bug report 
the most, followed by well-written, grammar error free summaries and 
descriptions. Last but not least, they also created a tool called CueZilla that 
could with an accuracy rate between 31 and 48\% predict the quality of a bug 
report. 

Strengthening the argument that readability matters considerably in bug report 
quality is the work of \citet{hooimeijer2007modeling}. They ran an analysis over
27000 bug reports from the Mozilla Firefox project, looking at self-reported 
severity, readability, daily load, submitter reputation and changes over time. 
After running the evaluation, they not only found out about the importance of
readability in bug reports, but also that attachment and comment counts are 
valuable for faster triaging and that patch count, for example, does not
contribute in the same manner as the previous two.

Another research that agrees that stack traces are helpful in solving bug
reports faster is the one performed by \citet{schroter2010stack}. They conducted
the whole experiment on the Eclipse project. They first extracted the stack
traces using the infoZilla tool(\citep{bettenburg2008extracting}), followed by
linking the stack traces to changes in the source code (change logs) by mining
the version repository of the Eclipse project. The results showed that 
around 60\% of the bugs that contained stack traces in their reports were fixed
in one of the methods in the frame, with 40\% being fixed in exactly the first
stack frame.

After examining the work of \citet{bettenburg2007quality}, we noticed that
the authors found very similar results to the ones presented in the research of \citet{bettenburg2008makes}. 
They performed the same type of evaluation method (i.e. 
interviews with developers) and confirmed that steps to reproduce and stack traces
are the most important features in a bug report that help developers solve the
issue quicker.

Other interesting findings regarding the quality of a software project's tickets
are the ones elicited in \citet{bettenburg2008duplicate}. The authors examined
whether duplicate bug reports are actually harmful to a project or they add 
quality. They collected big amount of data from the Eclipse project and ran
various kind of textual and statistical analysis on the data to find answers.
They reached the conclusion that bug duplicates contain information that is not
present in the master reports. This additional data can be helpful for developers
and it can also aid automated triaging techniques.

However, in order to model the quality of a bug report, one needs to be able to
successfully extract various types of information from such a report and, if 
possible, link them to the source code of the project (i.e. source code fragments
in bug report discussions should be linked to the corresponding sections in the 
actual code) or other software artifacts. There are several researches that tried 
to analyze such techniques and one of them is the work of 
\citet{bettenburg2012using}. The authors created a tool that could parse a bug 
report (using fuzzy code search) and extract source code that could then be
matched with exact locations in the source code, in the end producing a 
traceability link (i.e. tuple containing a clone group ID and file paths 
corresponding to the source code files). Evaluation showed an increase of
roughly 20\% in total traceability links between issue reports and source code 
when compared to the current state-of-the-art technique, change log analysis.

\citet{bettenburg2012using} also made use of a tool developed by 
\citet{bettenburg2008extracting} called infoZilla. This application can
parse bug reports and correctly extract patches, stack traces, source code and
enumerations. When evaluating it on over 160.000 Eclipse bug reports, it proved to 
have a very high rate of over 97\% accuracy.

However, \citet{kim2013should} propose a rather different technique than the one
exposed in \citet{bettenburg2012using}. The authors employed a two-phase
recommendation model that would locate the necessary fixes based on information
in bug reports. Firstly, they did a feature extraction on the bug reports (e.g. 
extract description, summary, metadata). Then, in order to successfully predict
locations, this model was trained on collected bug reports and then, when given
a new bug report, it would try to localize the files that need to be changed 
automatically. Finally, the actual implementation was put into place, and that
is the two phase recommendation model, composed of binary (filters out 
uninformative bug reports before predicting the files to fix) and multiclass (
previously filtered bug reports are used as training data and only after that
new bug reports can be analyzed and files to be changed recommended). The 
overall accuracy was above the one achieved by \citet{bettenburg2012using}, 
being able to rank over 70\%, but only as long as it recommended \emph{any} 
locations.

One other type of information that could be extracted and used for valuable
purposes is the summary of a bug report, as shown in the work of 
\citet{rastkar2010summarizing}. The authors wanted to determine if bug reports
could be summarized effectively and automatically so that developers would
need only analyze summaries instead of full tickets. Firstly, they asked
university students to volunteer to annotate the bug reports collected from
various open sources projects (i.e. Eclipse, Mozilla, Gnome, KDE) by writing
a summary of maximum 250 words in their own sentences. These human-produced 
annotations were then used by algorithms to learn how to effectively summarize a 
bug report.  Afterwards, the authors asked the end users of these bug reports, the 
software developers, to rate the summaries against the original bug reports.
They eventually learned that existing conversation-based extractive summary
generators trained on bug reports produce the best results.

A third technique for linking source code files from the textual description of a 
bug report is the one proposed by \citet{zhou2012should}. The authors
first performed a textual analysis, looking for similarities between the bug 
report description and the source code files. Then, it analyzed previous bugs in 
the repository to find the most similar ones, thus being able to find which files 
ought to be changed. Lastly, it assigned scores to similar files, the ones with 
bigger sizes obtaining higher scores as they are more likely to contain bugs. They 
collected the necessary data from Bugzilla projects and then performed the 
evaluation. The results of the evaluation phase were positive: the bug locator
can locate a large percentage of bugs analyzing just a small set of source code
files. Moreover, similar bugs can improve the localization accuracy only to a 
certain extent, while the locator outperformed every other competitor on a 
multitude of projects. 

Having discussed automated linkage between source code files and bug reports, 
\citet{fischer2003analyzing} presents a way to track certain features from the
data residing in tickets. The authors employed a method to track features by 
analyzing and relating bug report data filtered from a release history database.
Features are then instrumented and tracked, relationships of modification and 
problem reports to these features are established, and tracked features are
visualized to illustrate their otherwise hidden dependencies. They managed to 
successfully visualize the tracked features and illustrate their non apparent
dependencies which can prove very useful in projects with a large number of
components.

Even though we have discussed about quality in bug reports, how it can be 
identified and what state-of-the-art techniques can be used to extract and 
link various components from tickets, we must also investigate if the platform 
where these bug reports 
reside (i.e. bug trackers) are properly fitting the software environment. 
The work of \citet{just2008towards} tries to find the answer to this question.
The authors launched a survey to developers from Eclipse, Mozilla and Apache
open source projects from which they received 175 comments back. Then, they
applied a card sort in order to organize the comments into hierarchies to 
deduce a higher level of abstraction and identify common patterns. In the end,
they ranked the top suggestions as follows:
  \begin{itemize}
    \item provide tool support for users to collect and prepare information that 
      developers need;
    \item find volunteers to translate bug reports filed in foreign languages;
    \item provide different user interfaces for each user level and give cues to 
      inexperienced reporters on what to report and best practices;
    \item reward reporters when they do a good job;
    \item integrate reputation into user profiles to mark experienced reporters;
    \item provide powerful and easy to use tools to search bug reports;
    \item encourage users to submit additional details (provide tools for merging 
      bugs).
  \end{itemize}

One other important aspect when analyzing the quality of a bug report is the
time it takes to triage it. Triaging is the process of analyzing issues and 
based on the information available, determine their severity and assign them to the
most qualified available developer to fix them. If the ticket was valuable and had 
an increased index of quality, the triaging process would be quicker and,
thus, reduce the cost for the organization. There are several researches that 
are concerned with this field and how gaps can be bridged.

One such example is the work of \citet{anvik2006automating}. The authors
automated the process of bug report assignment by employing a machine learning 
algorithm which, after receiving as input various types of information (e.g. 
textual description of bug, bug component, OS, hardware, software version, 
developer who owns the code, current workload of developers, list of developers 
actively contributing), would recommend the best developers to work on the ticket. 
After the tool was evaluated, it was found that it could actually be deployed
in the software industry so that it could be used by organizations.

Working on the previous tool, the same authors proposed a new solution to automatic
bug report assignment through the work presented in \citet{anvik2011reducing}.
They increased the precision and recall of the algorithm and then, in order to 
test it, they implemented a proxy to the actual web service that's providing the
issue repository. Eventually, they also found out that the impact of poor
software project management, including bug triaging, on software projects can
reach alarming levels.

A similar research, but which looks at the free form of the ticket only, is
the technique presented in \citet{anvik2006should}. The authors applied a 
supervised machine learning algorithm on the repositories to learn which 
developers were best suited for specific tasks, thus when a new bug report would 
come in, a small set of people would be selected. In order to train the algorithm,
they looked at Bugzilla repositories and selected the free text form of tickets, 
trying to label similar ones based on textual similarities. Once the tickets were
labeled and grouped for specific developers, the algorithm would then be able
to present the triager the set of developers suitable to fix the bug. Even though
this approach, compared to \citet{anvik2006automating} or 
\citet{anvik2011reducing}, does not reach the same levels of accuracy, it is much
simpler to implement and evaluate.

However, a completely different approach is the one presented by 
\citet{jeong2009improving} that looks at both bug report assignment as well as bug 
tossing (i.e. re-assign tickets from one developer to another). They wanted, 
through their tossing graph approach based on the Markov property, to both 
discover developer networks and team structures, as well as help to better assign 
developers to bug reports. They analyzed 145,000 bug reports from Eclipse and 
300,000 from Mozilla and then, using statistical analysis on the bug reports in 
order to find evolution histories and changes throughout the lifetime of the 
reports, they created the bug tossing graph. The research shows that it takes a 
long time to assign and toss bugs and, additionally, they learned that their model 
reduces tossing steps by up to 72\% and improved the automatic bug assignment by 
up to 23\%.

Another approach to bug report assignee recommendation is the one based on
activity profiles, proposed by \citet{Naguib2013BugReportAssignee}. The authors 
employed an algorithm using activity profiles (i.e. assign, review, resolve 
activity of the developer) such that, after detecting the prior
experience, developer's role, and involvement in the project, it could
recommend who should fix a specific bug. The average accuracy was around
88\%, much higher than other techniques, including the ones mentioned previously. 

However, bug report assignment is not the only part of bug triaging that needs
investigation. \citet{lamkanfi2010predicting} show a method through which one
can predict the severity of a reported bug (i.e. assign story points to the 
ticket) automatically. The authors conducted their research on the Mozilla,
Eclipse and GNOME projects and divided their method into four steps:
  \begin{itemize}
    \item extract and organize bug reports;
    \item pre-process bug reports - tokenization, stop word removal and stemming;
    \item train the classifier on multiple datasets of bug reports - 70\% 
      training and 30\% evaluation;
    \item apply the trained classifier on the evaluation set.
  \end{itemize}
The conclusions they drew from running the evaluation process were rather 
interesting:
  \begin{itemize}
    \item terms such as deadlock, hand, crash, memory or segfault usually indicate
      a severe bug; however, there are other terms that can indicate the opposite,
      such as typos;
    \item when analyzing the textual description they found that using simple
      one line summaries resulted in less false positives and increased precision
      levels;
    \item the classifier needs a large dataset in order to predict correctly;
    \item depending on how well a software project is componentized, one can 
      use a predictor per component rather than an universal one.
  \end{itemize}

\subsection{Measuring Cost and Waste in Software Projects}\label{waste}

Having talked about data and ticket quality and its characteristics, we have 
another issue that inevitably arises in every software project: software waste. 
There are different researches that try to define what waste in software 
development is as well as its main types. One such paper is the one of 
\citet{sedano2017software}. The authors conducted a participant-observation 
study over a long period of time at Pivotal, a consultancy software development
company. They also interviewed multiple engineering and balanced theoretical 
sampling with analysis to elicit the following main types of waste in software
project:
  \begin{itemize}
    \item building the wrong feature or product;
    \item mismanaging backlog;
    \item extraneous cognitive load;
    \item rework;
    \item ineffective communication;
    \item waiting/multitasking;
    \item solutions too complex;
    \item psychological distress.
    \item \textbf{Relevance to our work}: this paper complements the previous 
      one on waste identification\cite{Korkala2014WasteIdentification}.
  \end{itemize}

Even though \citet{sedano2017software} look at waste generally in software 
development, \citet{ikonen2010exploring} try to specifically investigate waste
in kanban-based projects. They controlled a case study research at a company 
called Software factory using semi-structured interviews. The authors reached
two main conclusions:
  \begin{itemize}
    \item they couldn't explain the success of the project even though waste was 
      found;
    \item they identified 7 main types of waste throughout various development
      stages: partially done work, extra processes, extra features, task 
      switching, waiting, motion and defects.
  \end{itemize}
Apart from building the wrong feature/product, the 2 works differ in what types
of waste they identified.

However, defining waste and its main types is not enough - one needs also
ways to identify such waste. \citet{Korkala2014WasteIdentification} propose a
technique to identify communication waste in agile software projects 
environments. The authors collaborated with a medium-sized American software 
company and conducted a series of observations, informal discussions, documents 
provided by the organization, as well as semi-structured interviews. Moreover,
the data collection for waste identification was split into 2 parts:
  \begin{itemize}
    \item \textbf{pre-development}: occurred before the actual implementation
      begun (e.g.\ backlog creation);
    \item \textbf{development}: happened throughout the implementation
      process (e.g.\ throughout sprints, retrospectives, sprint reviews,
      communication media).
  \end{itemize}
Eventually they reached two main conclusions:
  \begin{itemize}
    \item this proposed approach was efficient and the authors recommend it to
      companies if they'd like to conduct such processes internally;
    \item the research elicits 5 main categories for communication waste: lack
      of involvement, lack of shared understanding, outdated information, 
      restricted access to information, scattered information.
  \end{itemize}

Another work that is trying to analyze the waste around software coordination activities
is the one of \citet{aranda2009secret}. They are trying to understand common bug fixing
coordination activities in terms of software projects coordination and propose different
directions on how to implement proper tools. They conducted a field study which was
split into two parts:
  \begin{itemize}
    \item led an exploratory case study of bug repositories histories;
    \item then, they conducted a survey with professionals (e.g. testers, developers).
  \end{itemize}
After they finished the 2 phases, they learned that there are multiple factors which
influence the coordination activities that revolve around bug fixing, such as organizational,
social and technical knowledge, thus one cannot infer any conclusions only by automatic analysis 
of the bug repositories. Also, through surveying the professionals, they reached the conclusion
that there are \emph{eight main goals} which can be used for better tools and practices: summit, 
probing for ownership, probing for expertise, code review, triaging, rapid-fire emailing, 
shotgun emails and infrequent/direct email.

However, having discussed about software and communication waste, what work has been done
around another major software activity, i.e. software cost estimation? The paper from 
\citet{grimstad2006framework} showed that poor estimation analysis techniques
in software projects will lead to wrong conclusions regarding cost estimation accuracy.
Moreover, they also propose a framework for better analysis of software cost estimation
error. They approached a real-world company where they conducted analysis on their cost
estimation techniques. Eventually, they learned that regular, straight-forward types
of cost estimation analysis techniques errors lead to wrong conclusions, thus increasing
the risk of poor coordination activities.

But what happens when a project gains maturity? A work that investigates the effects of process
maturity on quality, cycle time and effort is the one undergone by \citet{harter2000effects}. The
research proves and rejects several hypotheses, such as higher levels of process maturity lead 
to higher product quality, higher levels of process maturity are associated with increased cycle time,
higher product quality is associated with lower cycle time, higher levels of process maturity lead 
to increased development efforts in the project and higher product quality is associated with lower
development effort. The authors examined data coming from 30 projects belonging to the systems 
integration division from a large IT company and analyzed a couple of key variables and see how
they interact (i.e. process maturity, product quality, cycle time, development effort, product
size, domain/data/decision complexity and requirements ambiguity). They learned that the main
features they inspected are additively separable and linear. Moreover, they found out that 
higher levels of process maturity are associated with significantly higher quality, but also
with increased cycle times and development efforts. On the other hand, the reductions in cycle
time and effort resulting from improved quality outweigh the marginal increases from achieving
more process maturity. 

As we want to analyze comments as well and how they influence the quality of a ticket, we
need to investigate what state-of-the-art tools are available to perform sentiment analysis.
One paper that tried to detect the overall sentiment transmitted through reviews of various 
types is the research performed by \citet{turney2002thumbs}. The author created an unsupervised
machine learning algorithm that was evaluated on more than 400 reviews on Epinions on various
kins of markets. The algorithm implementation was divided into three steps:
  \begin{itemize}
    \item extract phrases containing adjectives or adverbs;
    \item estimate the semantic orientation of the phrases;
    \item classify the review as recommended or not recommended based on
      the semantic orientation calculated at previous step.
  \end{itemize}
The author learned that different categories will yield different results, such as the
automobile section on Epinions ranked much higher, 84\%, compared to movie reviews, which
had an accuracy of 65.83\%. Moreover, most pitfalls of the algorithm could be attributed
to multiple factors, such as not using a supervised learning system or limitations of 
PMI-IR.

However, the work of \citet{turney2002thumbs} looks only at sentiment analysis without
examining the context. The research from \citet{wilson2005recognizing} found
efficient ways to distinguish between contextual and prior polarity. They used a two 
step method that used machine learning and a variety of features. The first step 
classified each phrase which had a clue as either neutral or polar, followed by 
taking all phrases marked in the previous step and giving them a contextual polarity 
(e.g. positive, negative, both, neutral). Through the method the authors employed,
they managed to automatically identify the contextual polarity. As most papers were only
looking at the sentiment extracted from the overall document, they managed to get 
valuable results from looking at specific words and phrases.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proposed Approach}\label{proposed_approach}

\subsection{Experimental Design}

We would like to begin the research by conducting a statistical analysis 
over popular open source projects such as Mozilla or Apache and extract 
tickets most probably via REST APIs, however the list of projects
will be finalized once some tests will be run, and that is because
maybe some projects will have more valuable tickets for us to analyze and
we want to analyze multiple issue tracking systems 
(e.g. Apache uses Jira \citep{jira} while Mozilla uses Bugzilla 
\citep{bugzilla}). 

Then, as most issue tracking systems respond with JSON payloads through 
their REST APIs, we can easily extract components such as summary or 
description and analyze them. We have defined that the initial list of 
components that we want to inspect into three main categories:
  \begin{itemize}
    \item regular text - summary, description, story points, 
      text in comments, status (e.g. OPEN, WONTFIX, CLOSED);
    \item non-regular text (i.e. special structures) - stack traces,
      steps to reproduce;
    \item non-text - attachments in comments (e.g. images, screenshots
      when trying to show steps to reproduce).
  \end{itemize}

Extracting text from components is rather trivial, but extracting and 
analyzing non-text data such as images or non-regular text will be 
more complex. 

For images, there are two very popular REST APIs that 
we can use for analysis: Google Cloud Vision \citep{vision} and 
AWS Rekognition \citep{rekognition}. When an image is sent via the API,
the servers respond with detailed information about what can be found
in the image, thus we can detect if the image was for example a 
simple screenshot or if it contained multiple screenshots that were aimed
at visually showing steps to reproduce a bug.

In terms of non-regular text, steps to reproduce are usually trivial to
retrieve (if they are not images, for which we have presented a solution
in the above paragraph), they will most probably be written as bullet
lists or a sequence of lines starting with \emph{Step [number] ...}, thus
a simple regex will probably suffice. However, for stack traces, as they
are more complex, there are no commercial or open source applications
that could help them extract them easily, thus we will compose a special
regex depending on the language of the project (e.g. Apache heavily 
uses Java, thus we can look for lines starting with java.*.*Exception ...).

Implementation wise, we would like to develop the application that will
perform both scraping, extraction and inspection in either Go or Java.

In order to detect what features make for a high quality ticket, we need 
to compare two \emph{similar} tickets and then analyze their components
for differences. Thus, one of the biggest issues we will be facing is 
comparing and contrasting tickets. We have two initial approaches
that we want to test to see which one is more efficient:
  \begin{itemize}
    \item look solely at the issue tracking system and use story points
      to detect similar tickets; one limitation of this approach is that
      either the triager is not experienced and might assign wrong story
      point values or the initial story point values are not properly
      assigned and they will be corrected during the ticket's lifecycle;
    \item look at the version control system and compare changesets for
      similar work effort; however, this approach has one limitation - 
      solely looking at source code changes might not encapsulate the
      whole effort needed to solve the task (e.g. the developers on the
      team might spend extra hours only on designing the fix).
  \end{itemize}
However, this list is not final, so if we find some other method 
while running the experiment, we shall switch to that.

Finally, grouping similar tickets together allows testing 
the variances in component features they possess and see which features
affected the time-to-close the most.

\subsection{Variables Tested}

The independent variables proposed for testing are:
  \begin{itemize}
    \item presence and number of stack traces in a ticket;
    \item steps to reproduce (as described above - images or text);
    \item presence of screenshots in tickets;
    \item text components (e.g. summary, description) being either 
      succinct or detailed;
    \item grammar-free textual description;
    \item overall sentiment drawn from comments (e.g. angry or calm);
    \item complex or light discussion in comments.
  \end{itemize}

In terms of dependent variables, there is only one - the time it takes
between when the ticket was created and marked as OPEN until when it
is finished and marked as CLOSED or RESOLVED.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Work Plan}\label{work_plan}

The work on the actual project in the second semester will start off 
with deciding and analyzing which repositories to use for ticket data.
There are several popular open source options, such as Apache, Mozilla or
Eclipse, so settling on a couple of options will be the first step in our
research. We believe that this step will be finished \emph{by the end of
January}.

The next step will be to create the scraping, extraction and inspection
tool which will be implemented in either Go or Java. This will be a vital
component of the whole research so we believe that it will be ready to run
\emph{by the end of February}.

Then, concomitantly with the previous work package, the method used to 
detect similar tickets will be decided on. This step will be performed
during the creation of the tool because efficiency can only be evaluated
once ideas have been validated via execution of several mock tickets. 
Thus, the proposed deadline for this work package is \emph{the end of February} as well.

Following, tickets collected will be run in the tool created as per the previous steps. 
The expected deadline is \emph{by 4\textsuperscript{th} of March}.

Then, the actual write-up of the paper will be structured as follows:
  \begin{itemize}
    \item write abstract, introduction, related work - this is expected
      to be completed \emph{by mid March};
    \item write methodology (collecting
      data, running analysis) - expected to be finished \emph{by the 
      beginning of April};
    \item write results, contributions and conclusion for the paper - 
      we expect to complete it \emph{by 18\textsuperscript{th} of April}.
  \end{itemize}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{plainnat}
\bibliography{mprop}
\end{document}
