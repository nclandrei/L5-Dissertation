\documentclass{mpaper}

\begin{document}

\title{Measuring Software Ticket Quality using Quantitative Data Analysis}
\author{Andrei-Mihai Nicolae}
\matricnum{21473292}

\maketitle

\begin{abstract}
Software tickets are of valuable importance to the whole computing science 
field - they guide engineers towards better planning, management 
and tracking of their progress throughout complex projects. However, there
are few studies that investigate what makes for a high quality,
valuable ticket. This can lead to multiple issues in a company, such as 
increased communication friction between developers and end users filing bug
reports, as well as increased overall costs due to waste of development effort. 
In this research paper, we present our findings after 
investigating a large number of variables surrounding software tickets, 
such as whether the presence of stack traces influence the time 
to close for the ticket. Our results show that the presence and type of attachments,
comments complexity (i.e. number of comments per ticket and total number of words), 
summary and description complexity, grammar correctness scores
as well as the sentiment drawn from the comments can influence the quality of the ticket.
We bring a couple of novel aspects to the research
community including one of the largest dataset statistically analysed in the field,
as well as state-of-the-art sentiment and grammar correctness analysis.
\end{abstract}

\section{Introduction}

In the past decade, technology has drastically increased its influence on 
virtually every aspect of our society. Therefore, software projects have 
inherently become more complex and require increasing number of developers in
the team. Due to this, software engineers have created issue tracking systems,
a means of planning, managing and tracking work products in the form of 
\emph{software tickets} or \emph{issues}.

There are multiple platforms for providing such issue tracking systems, among which
the most popular are Jira \cite{jira} and Bugzilla \cite{bugzilla}. For both platforms,
the tickets are split into two main categories: feature requests (i.e. feature to be 
implemented into the system) and bug reports (i.e. issue encountered by an end user or
discovered by a developer in the codebase). Regardless of the type of ticket, they possess
various information that can be filled in by the reporter (i.e. person who created the ticket; 
can be both an end user or a developer in the team), providing the developers
a detailed view of what is requested or what went wrong.

Even though tickets provide such comprehensive data regarding a specific task, studies have shown 
that fixing bugs is one of the most common tasks performed by developers \cite{latoza2006maintaining}. One of
the reasons for this is the communication friction between developers and end users \cite{Korkala2014WasteIdentification}
as developers might need clarification regarding what information the users have provided (e.g. cannot reproduce the bug, 
screenshot is unclear). Another main reason for this waste of effort on solving tickets, according to 
Just et. al \cite{just2008towards} and Zimmermann et. al \cite{zimmermann2009improving}, is the generally poor design of issue 
tracking systems. This can lead to various issues, including increased costs for the company, wasted development effort, 
decreased customer satisfaction and overall poor performance.

Therefore, there is a need in the community to find the answer to what makes for a high quality, valuable software ticket 
that would improve the overall performance of the development team and, inherently, the company. As there are many 
fields in a ticket, there are numerous unanswered questions, such as whether stack traces have an influence on the quality
of a ticket? 

In this research paper, we present and discuss our findings after running a quantitative data analysis 
on over 3200,000 tickets taken from more than 15 open source projects. We have implemented a Go application 
with multiple commands (i.e. store, analyze, plot, statistics) that can automatically fetch any number of tickets 
from a Jira instance, analyze them, generate plots and run statistical tests. 

During the analysis part, we investigate several variables in correlation with \emph{Time-To-Close}, 
which we define as the \emph{metric of quality}. \emph{Time-To-Close} represents the period of time between the creation 
and the closing of a ticket; more specifically, the creation of the ticket is marked when the status of the ticket is set to 
\emph{Open} and the closing of a ticket is considered when the ticket status is set to \emph{Closed} (or some similar status, 
such as \emph{Fixed}, \emph{Resolved}). \emph{Time-To-Close} is our dependent variable, and as independent variables 
(i.e. variables controlled in order to test the effects on the dependent variable) we have set a number of ticket fields. 

In order to provide answers to what factors influence quality in a software ticket, we answer the following seven research 
questions in this paper (Section \ref{correlations}):
\begin{itemize}
  \item does the presence of attachments and their type (e.g. code snippet, screenshot) influence the \emph{Time-To-Close} for 
  a ticket?
  \item does the presence of stack traces improve the ticket quality?
  \item does the presence of steps to reproduce reduce the \emph{Time-To-Close}?
  \item is there a relationship between the number of comments influence \emph{Time-To-Close}?
  \item is there a relationship between 
\end{itemize}

This study brings several contributions to the research community:
\begin{itemize}
  \item an innovative tool was built for the purpose of this project, its strengths lying in its simplicity, speed and
  extensability;
  \item it is one of the few studies in the field that performs a quantitative analysis rather than a qualitative one;
  \item it is one of the very few research projects that investigates such a large number of tickets (over 300,000)
  extracted from 38 different projects;
  \item it is, to our knowledge, the first study to conduct sentiment and grammar correctness analyses on software tickets.
\end{itemize}

In Section \ref{related_work} we iterate over state-of-the art studies in the field and then continue with 
Section \ref{building} where we discuss how the ticket data set was collected and analysed. We continue with describing 
the data set (Section \ref{characterising}) where we provide insights into various aspects of the data (e.g. size of database,
number of tickets with attachments) and then discuss about correlations between the variables (Section \ref{correlations}).
Finally, we provide future research directions in Section \ref{future_work} and present our conclusions in Section \ref{conclusions}.

\section{Related Work}\label{related_work}

The study of Bettenburg et. al \cite{bettenburg2008makes} showed what 
makes for a valuable bug report through qualitative analysis. 
After conducting interviews with over 450 developers, one of the main factors
behind a quality ticket is grammar correctness in summary and description. Another
aspect which was flagged as helpful by the interviewees was the presence of stack 
traces and steps to reproduce in tickets. A further contribution brought by the
authors to the community is the creation of a tool called Cuezilla which is able 
to automatically predict the quality of a bug report with an accuracy of around 40\%.

Another research paper that strenghtens the argument that readability is a quality 
factor in software tickets is the one presented by Hooimeijer et. al \cite{hooimeijer2007modeling}.
They conducted their analysis on over 25,000 bug reports from the Mozilla project, 
investigating readability, daily load, submitter reputation, the whole changelog histories 
and severity. They conclude that not only readability in the textual fields of a ticket
influence the \emph{Time-To-Close}, but also that the presence of attachments and 
the number of comments have a clear effect on the duration of triaging. On the 
other hand, the patch count and other similar fields did not provide significant 
value to the quality of tickets.

However, there are other types on information typically included in software tickets 
which might prove beneficial for developers and subsequently improve the ticket quality.
One such type is stack traces and Schroter et. al \cite{schroter2010stack} analyse how 
quickly tickets are closed when they either have stack traces included in their fields 
(e.g. description) or not. They collected their data from the Eclipse project using 
the InfoZilla tool proposed by Bettenburg et. al \cite{bettenburg2008extracting}.
Then, they linked the stack traces to changes in source code by mining the Eclipse version 
controlled repository. The results showed that 
around 60\% of the bugs that contained stack traces in their reports were fixed
in one of the methods in the frame. Moreover, more than 40\% of the tickets having stack traces
got fixed in the first stack frame.

Software tickets can have duplicates, usually meaning that the most important fields, such 
as summary or description, describe the same feature request or bug report. Even though 
one might believe that they cannot bring any value to a software project, the work of 
Bettenburg et. al \cite{bettenburg2008duplicate} shows the contrary.
The authors collected large amounts of data from the Eclipse open source project and ran
various kind of textual and statistical analysis on the data to find answers.
After the results were computed, they concluded that usually bug duplicates contain 
information that is not present in the master report (i.e. the original report that was 
filed). Moreover, developers also specified that they have often found value in these 
duplicate bug reports and that they can even aid automated triaging techniques.

Bettenburg et. al \cite{bettenburg2012using} present in their work an application
called infoZilla. This tool can
parse bug reports and correctly extract stack traces, patches and source code. 
When evaluated on over 150,000 bug reports, it proved to have a very high rate of over 97\% accuracy.
We applied some of the techniques shown in the study and successfully managed to 
retrieve stack traces as well with a great accuracy.

The first information to be filled in by end users or developers on any issue tracking system
is the summary field, which holds a small description, usually between 5 and 30 words, of 
the request or bug being reported. In the study conducted by Rastkar et. al \cite{rastkar2010summarizing}, 
the authors investigated whether tickets could be summarized automatically and efficiently. What 
that implies is that developers would not be required to look at complete tickets comprised 
of a large number of fields, but rather at a simple summary of a couple of lines 
encapsulating the whole information. The authors selected the Mozilla, Eclipse, Gnome and KDE 
open source projects and then they asked volunteering university students to annotate 
the tickets in the issue tracking systems. More specifically, they had to write a summary 
of maximum 250 words using both technical and non-technical terms, depending on their expertise. 
Then, the study presents Machine Learning algorithms employed to parse these summaries and 
learn how to efficiently create automated summaries for new bug reports. Afterwards, they asked 
software developers to rate these auto-generated summaries against the original ones and 
the conclusion was that existing conversation-based extractive summary generators used 
for software tickets produce the best results.

The work of Just et. al \cite{just2008towards} examines a rather different aspect
of quality in tickets - they are investigating the quality of the underlying issue 
tracking systems instead. 
The authors ran a survey on 175 developers from Eclipse, Apache and Mozilla
Then, they applied a card sort in order to organize the comments into hierarchies to 
identify common patterns. Their findings can be summarized in the following top seven
suggestions:
  \begin{itemize}
    \item create differentiation between bug report difficulty levels between 
    novices and experiences reporters;
    \item encourage users to input as much information as possible;
    \item add ability to merge tickets when needed;
    \item recognise and reward valuable bug reporters;
    \item integrate reputation into user profiles to mark experienced reporters;
    \item to be able to easily and expressively search through tickets.
  \end{itemize}

Having discussed about various factors that can influence the quality in software tickets, 
we also need to take into account how can we apply this knowledge to solving the issue of 
poor design in issue tracking systems and how the ticket creation process could be improved. 
The study conducted by Lamkanfi et. al \cite{lamkanfi2010predicting} looks at this aspect and
shows how issue tracking systems could be extended in order to automatically generate the severity 
of a ticket (e.g. assign story points to a Jira issue). The authors conducted their research on the Mozilla,
GNOME and Eclipse and projects and split their approach into four steps:
  \begin{itemize}
    \item extract and organize tickets;
    \item pre-process tickets (i.e. tokenization, stop word removal and stemming);
    \item train the classifier on two datasets of tickets - 70\% 
      training and 30\% evaluation;
    \item apply the trained classifier on the evaluation set.
  \end{itemize}
The conclusions they drew from running the evaluation process were rather 
interesting:
  \begin{itemize}
    \item terms such as segfault, deadlock, crash, hand or memory typically indicate
      a severe bug;
    \item when analyzing the textual description they found that using simple
      one line summaries resulted in less false positives and increased precision
      levels;
    \item the classifier needs a large number of tickets in order to predict correctly;
    \item depending on how well a software project is modularised, one can 
      use a predictor per module rather than a universal one.
  \end{itemize}

Another overall aspect of software tickets and issue tracking systems that could be improved through 
determining the quality factors for tickets is automatically assigning a bug report or a feature request 
to the most suitable developer (or team of developers). Anvik et. al \cite{anvik2011reducing}
propose a Machine Learning technique that could automatically assign a bug report to the developer 
with the most expertise in that specific area. Firstly, the algorithm takes as input various types of information from 
tickets: textual description and summary, operating system used when the bug occurred, developer 
who owns the source code, which developers have lighter workloads at that point in time etc. After the 
tool was evaluated, the authors concluded that it can be used in production as it provides a high 
rate of accuracy when assigning the developers, but it needs more data so that the model is trained 
properly.

\begin{figure*}
\begin{center}
\includegraphics[width=\textwidth]{images/flow.pdf}
\end{center}
\caption{\label{fig-eg}Application flow of the Go tool.}
\end{figure*}

\section{Building the Data Set}\label{building}
The first step towards building the data set was to create a tool that was able to execute all the commands that we required:
fetching the tickets from any Jira instance, storing them into some form of database, analyze the variables of interest, 
automatically plot the correlations between them and run statistical analysis on the data. After careful consideration, 
we decided that Go was the best way to go for various reasons:
  \begin{itemize}
    \item it is designed with simplicity in min, thus making it easier for others who might join the project to read 
    and understand the codebase;
    \item it is compiled, statically typed, which implies that it is a much faster candidate than other interpreted languages
    such as Ruby or Python;
    \item is is designed with concurrency in mind, thus it helps reduce times of execution and computing power considerably;
  \end{itemize}

Throughout the entire application, we tried to apply the UNIX philosophy of creating small applications that do one thing, but 
do it well. Therefore, we implemented clean and simple packages where we tried to use the standard library as much as possible
so that potential contributors coming to the project would find it easy to start working directly on the code. Also, we designed 
the tool with extensibility in mind, so that other database providers (e.g. CockroachDB) or issue tracking systems (e.g. Bugzilla) 
could be easily implemented - this was achieved using the elegant Go interfaces. 

We split our tool into four main commands: \emph{store}, \emph{analyse}, \emph{plot} and \emph{stats}. They all 
follow the flow shown in figure \ref{fig-eg} and complete the whole application cycle, in the end producing a database 
of analysed tickets, as well as plots and statistics for investigating correlations, saved on the filesystem.

In the following three subsections we describe how we designed and implemented the commands mentioned above.

\subsection{Fetch and Store}

This is the first command implemented in our tool - it is desgined to fetch, from any valid Jira URL, any number of tickets 
and then store everything inside an instance of BoltDB \cite{bolt}, which is a very simple yet powerful key-value pair 
database. The whole process is parallelized with the possibility of scaling even up to hundreds of goroutines
(i.e. lightweight threads) running concurrently. 

The decision of choosing Bolt in favor of well tested, more traditional databases such as Postgres or MySQL came naturally when 
we noticed that we did not need to query the database often, but rather get the whole array of tickets and manipulate them. Moreover, 
a MongoDB or Postgres DB would have required a running server. Even though the server could have been running on the local machine, 
Bolt instances are actually files with the extension \emph{db} saved on disk, thus it is trivial to create backups.

We have interacted with the Jira Server REST API which has very good documentation written by the Atlassian team. After getting
familiar with all the features the REST API offers, we have opted for getting paginated issues from the server, which means 
slicing the whole set of tickets into multiple smaller arrays to improve performance and reduce the load on the server. We also
specified to Jira exactly what set of issue fields to return and, eventually, while responses were retrieved from the instance,
we began storing them into our Bolt DB instance. 

The way we chose to store them was to set the issue key (i.e. unique identifier that differentiates a ticket from all others across all 
projects inside a Jira instance) as the key of the pair and the value is the JSON representation of the ticket. This helped us reduce 
the size of the database once we reached hundreds of thousands of tickets.

\subsection{Analyse}

Once the Bolt DB instance is completely populated with all the tickets we were interested in, the analyze command first 
fetches all the issues in the database. Then, it runs in parallel the \emph{seven types of analysis}:
\begin{itemize}
  \item \emph{attachments} - checks whether attachments influence Time-To-Close and also look at what types 
  of attachments (e.g. screenshots, archive, code snippets) influence quality the most;
  \item \emph{steps to reproduce} - performs a complex regex to detect steps to reproduce and then checks whether their presence
  influence the quality of a ticket;
  \item \emph{stack traces} - runs complex regex for detecting stack traces in either summary, description or comments 
  and verifies whether their presence influence Time-To-Close for the ticket;
  \item \emph{comments complexity} - loops through all comments and counts all words; then, the tool verifies whether 
  having many words or comments decreases or increases Time-To-Close for the ticket;
  \item \emph{fields complexity} - same analysis as comments complexity, but only for summary and description;
  \item \emph{grammar correctness} - it uses Azure Cloud Bing Spell Check API \cite{bing} to perform analysis on summary, 
  description and comments; after concatenating everything and making it compatible with the API allowed formats, the tool 
  receives back in the JSON payload not only the number of flagged tokens (i.e. grammar errors), but also their types 
  (e.g. unknown token, misspell);
  \item \emph{sentiment} - uses Google Cloud Platform's Natural Language Processing API \cite{gcp_nlp} to retrieve the 
  sentiment score for summary, description and comments; it first concatenates everything and makes it conform to Google's 
  API and then sends the request, receiving a score from -1 to 1 inclusive (-1 is completely negative, 1 is completely 
  positive).
\end{itemize}

Implementation wise, the application first checks Bolt for tickets and gets either all of them in memory (easier to parse, 
but heavy on resource utilization) or slices them into smaller arrays to get processed afterwards (harder to parse as it 
eventually requires re-creation of the whole array of tickets before inserting back into the database, but it is much 
lighter on computing power). After having the tickets available, we start looping through them and perform all 
analysis types mentioned above. Once some sort of value is computed (e.g. grammar correctness score), it gets set in 
its corresponding field in the ticket struct and subsequently is stored back in the database. We chose this approach because 
we do not want to run the analysis every time we want to plot correlations between variables, but rather have the data 
already available there. Moreover, for external analysis such as hitting Google Cloud Platform APIs, there are costs for 
running the analysis, thus in this way we drastically reduce overall costs for the project.

\subsection{Plot and Statistical Tests}

Afterwards, we have created two extra commands for automatically generating plots (i.e. scatter plots and bar charts) and 
running statistical tests on the collected data. 

For plotting the data, we first connect again to the Bolt database instance and then filter out only the issues that have 
the variable \emph{Time-To-Close} set (i.e. tickets marked as closed when they were fetched from the Jira instance). Afterwards,
we either save scatter plots or bar charts to disk. 

Also, in order to validate our findings, the statistics command fetches all the data from the database and runs two types of 
tests: Welch's T Test \cite{welch1947generalization} for analysing categorical data (e.g. has/does not have attachments)
and Spearman's rank correlation coefficient \cite{spearman1904proof} for investigating continuous data (e.g. how do different 
grammar correctness scores change \emph{Time-To-Close}).

\section{Characterising the Data Set}\label{characterising}

The data that we collected is stored, as previously mentioned, in a Bolt database instance which is around 6 GB in size. 
All tickets are stored as key-value pairs, where the key is the ticket's unique ID (e.g. KAFKA-100) and the value is the 
JSON encoded representation of the ticket. The tickets stored have the following fields available for investigation:
\begin{itemize}
  \item attachments - files or images attached to tickets;
  \item summary - short description of the ticket;
  \item description - more detailed information regarding what is requested or what bug was encountered;
  \item time estimate - estimated number of hours to complete the task (set by triagers or developers);
  \item time spent - time period between opening the ticket and a specific point in time;
  \item created timestamp;
  \item issue status - jira specific statuses, including Open, Closed, Awaiting Review, Patch Submitted;
  \item due date - optional deadline for when the ticket should get closed;
  \item comments - discussion around the ticket conducted by developers, end users, triagers;
  \item priority - it can range from low priority (minor) to critical/blocker;
  \item issue type - specific Jira field that specifies whether the ticket is either a bug, a feature request or a general task.
\end{itemize}

Another component of issues that we stored and proved to be crucial in our analysis is changelog histories together with their 
corresponding items. They represent the whole history of a ticket and can show status transitions, story points modifications, 
summary/description editing etc. What made them useful for our study was that we needed to see when tickets were marked closed and, 
as Jira does not provide this in a separate field, we looped through all changelog history items and saved when the transition to 
Closed or a similar status was made. 

There are other fields that can be configured inside Jira, including custom fields, but we did not collect them as they would not 
have been helpful in conducting the analysis. However, in addition to the fields we saved, we also stored grammar correctness scores, 
sentiment scores, whether they have steps to reproduce, if stack traces are present and number of words in comments, summary and description. 
Even though all of them, apart from grammar and sentiment scores, can be computed locally, we store them because of performance issues
due to the very large size of the database. 

In total, we have collected 303,138 tickets spanned across 38 projects from the Apache Software Foundation: Impala, Eagle, Groovy, Lucene,
Hadoop, Kafka, Apache Infrastructure project, Tika, Solr, ActiveMQ, ZooKeeper, Velocity, Tez, Storm, Stratos, CouchDB, Cassandra, 
Beam, Aurora, Bigtop, Camel, CarbonData, Cloudstack, Flex, Flink, Ignite, HBase, Mesos, Ambari, Cordova, Avalon, Atlas, 
Cactus, Flume, Felix, Geode, Ivy and Phoenix. These projects are using a large varieties of programming languages, ranging from Java, C, 
Go to Python and Ruby \cite{apache_projects}. Moreover, in terms of contributors, the projects we selected range from small teams of people
such as Tika to large numbers of developers spread across the globe, such as the people working on Kafka. We came to this final number 
and range of tickets and projects because of a couple of reasons: 
\begin{itemize}
  \item the study needed as much diversity as possible in order to correctly analyse and validate the data;
  \item these are the most important, up-to-date and contributed to Apache projects by the open source world;
  \item Apache is one of the few companies/foundations that use Jira exclusively for their projects and it is public (they do 
  provide a Bugzilla alternative, but it is rarely updated compared to Jira).
\end{itemize}

More specifically, we computed the following numbers for the tickets we collected:
\begin{itemize}
  \item out of the 303,138 tickets, 236,383 tickets have been closed (i.e. marked Closed, Resolved, Done or Completed) by the time we 
  fetched them from the Jira instance;
  \item 287,120 tickets are of high priority (i.e. marked as Blocker, Critical, Major or High);
  \item 103,397 tickets have attachments which we split in the following categories:
    \begin{itemize}
      \item 4,082 have code attachments (e.g. Go, Java, Python);
      \item 7,171 have image attachments (e.g. png, jpg/jpeg, gif);
      \item 164 have video attachments (e.g. mkv, avi, mp4);
      \item 13,990 have text attachments (e.g. txt, md, doc(x));
      \item 918 have config attachments (e.g. json, xml, yml);
      \item 2,491 have archive attachments (e.g. zip, tar.gz, rar, 7z);
      \item 280 have spreadsheet attachments (e.g. csv, xlsx);
      \item 80,015 have other attachments (i.e. any file extension not pre-defined by us in one of the above categories).
    \end{itemize}
  \item 270,907 tickets have comments;
  \item 39,988 tickets have steps to reproduce (i.e. sequence of steps specified by the bug reporter that would reproduce a certain bug); 
  \item 1,942 tickets have Java stack traces - we have applied the technique described by Bettenburg et. al 
  \cite{bettenburg2008extracting} for extracting structured data from bug reports and even though the method proved to be efficient, 
  the total number of stack traces is so small due to the fact that many projects are actually written in languages other than Java;
  \item 133,689 tickets have grammar correctness scores (i.e. number of grammar mistakes inside summary, description and comments) - 
  they have been collected using Microsoft Azure Bing Spell Check API which is one of the best grammar checking tool in the industry; 
  \item 157,047 tickets have sentiment scores (i.e. positive/negative sentiment on a scale from -1 to 1 drawn from summary, description
  and comments) - they have been collected using Google Cloud Platform Natural Language Processing APIs.
\end{itemize}

All these numbers are also stored inside the database in a different bucket for easier access in the future. Moreover, the statistical 
tests we ran on the tickets are saved as well in the database.

\section{Correlations}\label{correlations}

In this section we present our results to the research questions we set to answer. We have computed charts for all 
independent variables using either scatter plots for continuous variables (e.g. number of comments/time-to-close) 
or bar charts for categorical variables (e.g. presence of attachments). We also computed statistical analysis for 
all variables to test their significance and rank correlation coefficient and we will present each of them in their 
corresponding subsection. Lastly, we also discuss possible reasons why certain correlations occured.

\subsection{Attachments}

\begin{figure}[ht]
  \begin{center}
  \includegraphics[scale=0.23]{images/attachments.png}
  \end{center}
  \caption{\label{attachments}Attachments analysis.}
\end{figure}

Attachments are, as mentioned previously, files attached to software tickets that can take any form - from code 
snippets written in Java to tar archives. We have analyzed all tickets that were both of high priority tickets and
were marked as Closed by the time we collected the data. Then we split all these tickets into two main categories: 
with attachments and without attachments. Furthermore, the tickets that were marked as having attachments were split 
into the categories specified in Section \ref{characterising}. 

Using the plot package in our Go tool, we automatically generated the bar chart shown in Figure \ref{attachments}. 
As it can be seen, various types of attachments can produce completely different \emph{times-to-close}. For 
tickets with archive we can see the longest time-to-close mean while tickets having text attachments are at 
the other end of the spectrum. The smallest mean is for other attachments, which is represented by any file 
that does not have an extension specified in one of the other categories. 

We assume that image attachments are screenshots attached to the ticket in order to help developers see and 
subsequently reproduce the bug. Thus, we can clearly see that the difference between the means of time-to-close 
for tickets without attachments and tickets with screenshots is significantly in favor of the latter. This can 
indicate that developers indeed find screenshots helpful in solving the ticket faster.

Another insightful result is the fact that having text, config and spreadsheet attachments significantly 
reduces the time-to-close for the ticket. This can imply that developers find value in various steps to reproduce, 
config values used by users when the bug encountered or data in a spreadsheet format (e.g. csv) that can be 
found in such files and helps them lower the time-to-close for the ticket.

On the other hand, having no attachments does not necessarily mean that the ticket will get closed after 
a longer period of time. As it can be seen in the figure, the mean time-to-close value for tickets without 
attachments is somewhat neutral in the entire data set.

A Welch T Test was computed to test the difference between tickets with attachments and those 
without attachments. The analysis revealed significant difference with $p > 0.01$ with an
average of $1380.551$ hours for tickets with attachments and an average of $3180.882$ hours
for tickets without attachments. 

Thus, our analysis shows that having attachments in most formats, ranging from markdown files to PNG screenshots 
and snippets of code, can help developers solve the tickets faster. We believe that the solution of making attachments
provide the most value to the software project would be if issue tracking systems would make explicit to the reporters 
that uploading a screenshot or a code snippet can help reduce the time-to-close for the ticket.

\subsection{Steps to Reproduce}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.23]{images/steps_to_reproduce.png}
  \end{center}
  \caption{\label{steps}Steps to reproduce analysis.}
\end{figure}

A Welch T Test was computed to test the difference between tickets with attachments (i.e. all categories 
shown in the graph and those without attachments. The analysis revealed significant difference with 
\emph{p} < 0.01 with an average of \emph{X <change this>} for tickets with attachments and an average of 
\emph{Y <change this>} for tickets without attachments.

\subsection{Stack Traces}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.23]{images/stack_traces.png}
  \end{center}
  \caption{\label{stack_traces}Stack traces analysis.}
\end{figure}

A Welch T Test was computed to test the difference between tickets with attachments and those 
without attachments. The analysis revealed significant difference with \emph{p} < 0.01 with an
average of \emph{X <change this>} for tickets with attachments and an average of \emph{Y <change this>}
for tickets without attachments.

\subsection{Comments}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.23]{images/comment_complexity.png}
  \end{center}
  \caption{\label{comments}Comments complexity analysis.}
\end{figure}

A correlational analysis revealed a weak positive/negative (one of them, depending on context) relationship
with \emph{r} = \dots, \emph{p} = \dots.


\subsection{Summary and Description}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.23]{images/fields_complexity.png}
  \end{center}
  \caption{\label{fields}Fields complexity analysis.}
\end{figure}

A correlational analysis revealed a weak positive/negative (one of them, depending on context) relationship
with \emph{r} = \dots, \emph{p} = \dots.

\subsection{Grammar Correctness}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.23]{images/grammar_correctness.png}
  \end{center}
  \caption{\label{grammar}Grammar correctness score analysis.}
\end{figure}

A correlational analysis revealed a weak positive/negative (one of them, depending on context) relationship
with \emph{r} = \dots, \emph{p} = \dots.

\subsection{Sentiment Scores}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[scale=0.23]{images/sentiment_analysis.png}
  \end{center}
  \caption{\label{sentiment}Sentiment score analysis.}
\end{figure}

A correlational analysis revealed a weak positive/negative (one of them, depending on context) relationship
with \emph{r} = \dots, \emph{p} = \dots.


\section{Future Work}\label{future_work}

\section{Conclusions}\label{conclusions}

\vskip8pt \noindent
{\bf Acknowledgments.}
Tim, my parents, Corina.

\bibliographystyle{abbrv}
\bibliography{paper}

\end{document}